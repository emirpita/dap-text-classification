{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "martial-bubble",
      "metadata": {
        "id": "martial-bubble"
      },
      "source": [
        "# DAP seminarski"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZxoV0-5VrjG",
        "outputId": "87e467d3-2f6a-4bd0-cca1-d14ceab53ee1"
      },
      "id": "AZxoV0-5VrjG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "comic-large",
      "metadata": {
        "id": "comic-large"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import xml.sax.saxutils as saxutils\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from pandas import DataFrame\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "automotive-tiffany",
      "metadata": {
        "id": "automotive-tiffany"
      },
      "outputs": [],
      "source": [
        "# Set Numpy random seed\n",
        "random.seed(1000)\n",
        "\n",
        "# Newsline folder and format\n",
        "data_folder = '/content/gdrive/MyDrive/reuters21578/'\n",
        "\n",
        "sgml_number_of_files = 22\n",
        "sgml_file_name_template = 'reut2-NNN.sgm'\n",
        "\n",
        "# Category files\n",
        "category_files = {\n",
        "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
        "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
        "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
        "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
        "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
        "}\n",
        "\n",
        "# Word2Vec number of features\n",
        "num_features = 400\n",
        "# Limit each newsline to a fixed number of words\n",
        "document_max_num_words = 90\n",
        "# Selected categories\n",
        "selected_categories = ['pl_usa']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "opponent-burke",
      "metadata": {
        "id": "opponent-burke"
      },
      "outputs": [],
      "source": [
        "# Create category dataframe\n",
        "\n",
        "# Read all categories\n",
        "category_data = []\n",
        "\n",
        "for category_prefix in category_files.keys():\n",
        "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
        "        for category in file.readlines():\n",
        "            category_data.append([category_prefix + category.strip().lower(), \n",
        "                                  category_files[category_prefix][0], \n",
        "                                  0])\n",
        "\n",
        "# Create category dataframe\n",
        "news_categories = DataFrame(data=category_data, columns=['Name', 'Type', 'Newslines'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "macro-relay",
      "metadata": {
        "id": "macro-relay"
      },
      "outputs": [],
      "source": [
        "def update_frequencies(categories):\n",
        "    for category in categories:\n",
        "        index = news_categories[news_categories.Name == category].index[0]\n",
        "        f = news_categories._get_value(index, 'Newslines')\n",
        "        news_categories._set_value(index, 'Newslines', f+1)\n",
        "    \n",
        "def to_category_vector(categories, target_categories):\n",
        "    vector = np.zeros(len(target_categories)).astype(np.float32)\n",
        "    \n",
        "    for i in range(len(target_categories)):\n",
        "        if target_categories[i] in categories:\n",
        "            vector[i] = 1.0\n",
        "    \n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "legitimate-allah",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "legitimate-allah",
        "outputId": "4422ec7e-c96a-4b34-a432-fd743d04b8b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file: reut2-000.sgm\n",
            "Reading file: reut2-001.sgm\n",
            "Reading file: reut2-002.sgm\n",
            "Reading file: reut2-003.sgm\n",
            "Reading file: reut2-004.sgm\n",
            "Reading file: reut2-005.sgm\n",
            "Reading file: reut2-006.sgm\n",
            "Reading file: reut2-007.sgm\n",
            "Reading file: reut2-008.sgm\n",
            "Reading file: reut2-009.sgm\n",
            "Reading file: reut2-010.sgm\n",
            "Reading file: reut2-011.sgm\n",
            "Reading file: reut2-012.sgm\n",
            "Reading file: reut2-013.sgm\n",
            "Reading file: reut2-014.sgm\n",
            "Reading file: reut2-015.sgm\n",
            "Reading file: reut2-016.sgm\n",
            "Reading file: reut2-017.sgm\n",
            "Reading file: reut2-018.sgm\n",
            "Reading file: reut2-019.sgm\n",
            "Reading file: reut2-020.sgm\n",
            "Reading file: reut2-021.sgm\n"
          ]
        }
      ],
      "source": [
        "# Parse SGML files\n",
        "document_X = {}\n",
        "document_Y = {}\n",
        "\n",
        "def strip_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def unescape(text):\n",
        "    return saxutils.unescape(text)\n",
        "\n",
        "# Iterate all files\n",
        "for i in range(sgml_number_of_files):\n",
        "    if i < 10:\n",
        "        seq = '00' + str(i)\n",
        "    else:\n",
        "        seq = '0' + str(i)\n",
        "        \n",
        "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
        "    print('Reading file: %s' % file_name)\n",
        "    \n",
        "    with open(data_folder + file_name, 'rb') as file:\n",
        "        content = BeautifulSoup(file.read().lower())\n",
        "        \n",
        "        for newsline in content('reuters'):\n",
        "            document_categories = []\n",
        "            \n",
        "            # News-line Id\n",
        "            document_id = newsline['newid']\n",
        "            \n",
        "            # News-line text\n",
        "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
        "            document_body = unescape(document_body)\n",
        "            \n",
        "            # News-line categories\n",
        "            topics = newsline.topics.contents\n",
        "            places = newsline.places.contents\n",
        "            people = newsline.people.contents\n",
        "            orgs = newsline.orgs.contents\n",
        "            exchanges = newsline.exchanges.contents\n",
        "            \n",
        "            for topic in topics:\n",
        "                document_categories.append('to_' + strip_tags(str(topic)))\n",
        "                \n",
        "            for place in places:\n",
        "                document_categories.append('pl_' + strip_tags(str(place)))\n",
        "                \n",
        "            for person in people:\n",
        "                document_categories.append('pe_' + strip_tags(str(person)))\n",
        "                \n",
        "            for org in orgs:\n",
        "                document_categories.append('or_' + strip_tags(str(org)))\n",
        "                \n",
        "            for exchange in exchanges:\n",
        "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
        "                \n",
        "            # Create new document    \n",
        "            update_frequencies(document_categories)\n",
        "            \n",
        "            document_X[document_id] = document_body\n",
        "            document_Y[document_id] = to_category_vector(document_categories, selected_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "hollow-raising",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "hollow-raising",
        "outputId": "b616afb4-98c6-4fc7-fe09-b931eadba40e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Name           Type  Newslines\n",
              "296           pl_usa         Places      12542\n",
              "35           to_earn         Topics       3987\n",
              "0             to_acq         Topics       2448\n",
              "293            pl_uk         Places       1489\n",
              "219         pl_japan         Places       1138\n",
              "166        pl_canada         Places       1104\n",
              "73       to_money-fx         Topics        801\n",
              "28          to_crude         Topics        634\n",
              "45          to_grain         Topics        628\n",
              "302  pl_west-germany         Places        567\n",
              "126         to_trade         Topics        552\n",
              "55       to_interest         Topics        513\n",
              "191        pl_france         Places        469\n",
              "587            or_ec  Organizations        349\n",
              "158        pl_brazil         Places        332\n",
              "130         to_wheat         Topics        306\n",
              "108          to_ship         Topics        305\n",
              "145     pl_australia         Places        270\n",
              "19           to_corn         Topics        254\n",
              "172         pl_china         Places        223"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99ef34c3-2644-4940-8b9b-b477cb3e17a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Type</th>\n",
              "      <th>Newslines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>pl_usa</td>\n",
              "      <td>Places</td>\n",
              "      <td>12542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>to_earn</td>\n",
              "      <td>Topics</td>\n",
              "      <td>3987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>to_acq</td>\n",
              "      <td>Topics</td>\n",
              "      <td>2448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>pl_uk</td>\n",
              "      <td>Places</td>\n",
              "      <td>1489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>pl_japan</td>\n",
              "      <td>Places</td>\n",
              "      <td>1138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>pl_canada</td>\n",
              "      <td>Places</td>\n",
              "      <td>1104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>to_money-fx</td>\n",
              "      <td>Topics</td>\n",
              "      <td>801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>to_crude</td>\n",
              "      <td>Topics</td>\n",
              "      <td>634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>to_grain</td>\n",
              "      <td>Topics</td>\n",
              "      <td>628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>pl_west-germany</td>\n",
              "      <td>Places</td>\n",
              "      <td>567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>to_trade</td>\n",
              "      <td>Topics</td>\n",
              "      <td>552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>to_interest</td>\n",
              "      <td>Topics</td>\n",
              "      <td>513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>pl_france</td>\n",
              "      <td>Places</td>\n",
              "      <td>469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>or_ec</td>\n",
              "      <td>Organizations</td>\n",
              "      <td>349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>pl_brazil</td>\n",
              "      <td>Places</td>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>to_wheat</td>\n",
              "      <td>Topics</td>\n",
              "      <td>306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>to_ship</td>\n",
              "      <td>Topics</td>\n",
              "      <td>305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>pl_australia</td>\n",
              "      <td>Places</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>to_corn</td>\n",
              "      <td>Topics</td>\n",
              "      <td>254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>pl_china</td>\n",
              "      <td>Places</td>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99ef34c3-2644-4940-8b9b-b477cb3e17a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99ef34c3-2644-4940-8b9b-b477cb3e17a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99ef34c3-2644-4940-8b9b-b477cb3e17a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "news_categories.sort_values(by='Newslines', ascending=False, inplace=True)\n",
        "news_categories.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "forced-office",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "forced-office",
        "outputId": "0f561735-6c05-4936-d808-3c62c4b6dc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Load stop-words\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize tokenizer\n",
        "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
        "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenized document collection\n",
        "newsline_documents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "strong-elephant",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "strong-elephant",
        "outputId": "58c30116-0585-46d7-8105-30a99e338b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "def tokenize(document):\n",
        "    words = []\n",
        "\n",
        "    for sentence in sent_tokenize(document):\n",
        "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
        "        words += tokens\n",
        "\n",
        "    return words\n",
        "\n",
        "# Tokenize\n",
        "for key in document_X.keys():\n",
        "    newsline_documents.append(tokenize(document_X[key]))\n",
        "\n",
        "number_of_documents = len(document_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "together-conservation",
      "metadata": {
        "id": "together-conservation"
      },
      "outputs": [],
      "source": [
        "# Create new Gensim Word2Vec model\n",
        "w2v_model = Word2Vec(newsline_documents, size=num_features, min_count=1, window=10, workers=cpu_count())\n",
        "# w2v_model.init_sims(replace=True)\n",
        "w2v_model.save(data_folder + 'reuters.word2vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "assisted-principle",
      "metadata": {
        "id": "assisted-principle"
      },
      "outputs": [],
      "source": [
        "# Load an existing Word2Vec model\n",
        "w2v_model = Word2Vec.load(data_folder + 'reuters.word2vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "judicial-virginia",
      "metadata": {
        "id": "judicial-virginia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98243a0c-2f85-456a-b641-2216d3d6ad1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "num_categories = len(selected_categories)\n",
        "X = np.zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(np.float32)\n",
        "Y = np.zeros(shape=(number_of_documents, num_categories)).astype(np.float32)\n",
        "\n",
        "empty_word = np.zeros(num_features).astype(np.float32)\n",
        "\n",
        "for idx, document in enumerate(newsline_documents):\n",
        "    for jdx, word in enumerate(document):\n",
        "        if jdx == document_max_num_words:\n",
        "            break\n",
        "            \n",
        "        else:\n",
        "            if word in w2v_model:\n",
        "                X[idx, jdx, :] = w2v_model[word]\n",
        "            else:\n",
        "                X[idx, jdx, :] = empty_word\n",
        "\n",
        "for idx, key in enumerate(document_Y.keys()):\n",
        "    Y[idx, :] = document_Y[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "unnecessary-graduate",
      "metadata": {
        "id": "unnecessary-graduate"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "revised-potato",
      "metadata": {
        "id": "revised-potato"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_categories))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "auburn-berkeley",
      "metadata": {
        "id": "auburn-berkeley",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69e3ede-f3f8-4f60-9cf3-2e6e3b9a0f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "118/118 [==============================] - 62s 509ms/step - loss: 0.6805 - accuracy: 0.5822 - val_loss: 0.6814 - val_accuracy: 0.5766\n",
            "Epoch 2/5\n",
            "118/118 [==============================] - 59s 501ms/step - loss: 0.6796 - accuracy: 0.5832 - val_loss: 0.6815 - val_accuracy: 0.5766\n",
            "Epoch 3/5\n",
            "118/118 [==============================] - 59s 499ms/step - loss: 0.6799 - accuracy: 0.5832 - val_loss: 0.6814 - val_accuracy: 0.5766\n",
            "Epoch 4/5\n",
            "118/118 [==============================] - 59s 503ms/step - loss: 0.6797 - accuracy: 0.5832 - val_loss: 0.6815 - val_accuracy: 0.5766\n",
            "Epoch 5/5\n",
            "118/118 [==============================] - 60s 505ms/step - loss: 0.6796 - accuracy: 0.5832 - val_loss: 0.6816 - val_accuracy: 0.5766\n",
            "51/51 [==============================] - 9s 180ms/step - loss: 0.6816 - accuracy: 0.5766\n",
            "Score: 0.6816\n",
            "Accuracy: 0.5766\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=5, validation_data=(X_test, Y_test))\n",
        "\n",
        "# Evaluate model\n",
        "score, acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
        "    \n",
        "print('Score: %1.4f' % score)\n",
        "print('Accuracy: %1.4f' % acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "DAPseminarski.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}